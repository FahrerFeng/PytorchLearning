{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor \n",
    "Tensor is a very important data type in Pytorch (even in Tensorflow). It can be a scala,array or n-dimensional matrix. Tensor is in some circumstance the same as \"ndarray\" in Numpy, for example its characteristics, shape, dtype... The difference between two libraries is that the operation of tensor can be **accelerated** by GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up tensor\n",
    "1. Using Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# build a 5x3 matrix, no initialization\n",
    "x = t.Tensor(5, 3)\n",
    "# print the shape of tensor in form of tuple\n",
    "print(x.shape)\n",
    "# print the data type of the established tensor\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talking about tensor type\n",
    "We use .type() operation to get the type of a defined tensor. Pytorch set FloatTensor as default. We can use set_default_tensor_type to change the default type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "print(x.type())\n",
    "t.set_default_tensor_type(t.DoubleTensor) # Only valid for newly constructed tensors, the old ones retain the original data format\n",
    "print(t.tensor([1, 2.2]).type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Using distribution, random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor using uniform distribution 0~1:tensor([[0.3270, 0.6949, 0.2548],\n",
      "        [0.2707, 0.1513, 0.1833],\n",
      "        [0.4186, 0.6758, 0.1735],\n",
      "        [0.0754, 0.7741, 0.0341],\n",
      "        [0.0359, 0.2405, 0.1259]])\n",
      "Comparison between .size: torch.Size([5, 3]); and .shape: torch.Size([5, 3])\n",
      "Tensor using uniform distribution with integer value:tensor([[7, 5],\n",
      "        [3, 9]])\n",
      "Tensor using normal distribution N(0, 1): tensor([[-0.3397,  1.2896,  1.8661],\n",
      "        [-0.5032, -0.5382,  0.5253],\n",
      "        [-0.7063, -0.3635,  0.4330]])\n",
      "Tensor using normal distribution with self defined mean and standard deviation:tensor([-0.1499, -0.8441,  0.4788, -1.6035,  0.5667,  0.4058,  1.0035, -0.0165,\n",
      "        -0.0034, -0.1500])\n"
     ]
    }
   ],
   "source": [
    "t.set_default_tensor_type(t.FloatTensor)\n",
    "# use uniform distribution 0~1 to build the 2-d tensor\n",
    "x = t.rand(5, 3)\n",
    "print(\"Tensor using uniform distribution 0~1:{ten}\".format(ten=x))\n",
    "print(\"Comparison between .size: {si}; and .shape: {sha}\".format(si=x.size(), sha=x.shape)) # both return a tuple format, so we can use tuple operation\n",
    "\n",
    "# create tensor with integer value in interval 1~10, 2x2 dimension\n",
    "x = t.randint(1, 10, [2, 2])\n",
    "print(\"Tensor using uniform distribution with integer value:{ten}\".format(ten=x))\n",
    "\n",
    "# normal distribution N(0, 1), dimension as input\n",
    "x = t.randn(3, 3)\n",
    "print(\"Tensor using normal distribution N(0, 1): {ten}\".format(ten=x))\n",
    "\n",
    "# normal distribution with self defined mean and standard deviation, dimension is defined using mean and standarad deviation\n",
    "x = t.normal(mean=t.full([10], 0, dtype=t.float), std=t.arange(1, 0, -0.1))\n",
    "print(\"Tensor using normal distribution with self defined mean and standard deviation:{ten}\".format(ten=x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Build up tensor using numpy data or original python data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of a:int32; Data type of a_t:torch.int32\n",
      "Data type of b_ft:torch.float32; Data type of b_t:torch.float32\n"
     ]
    }
   ],
   "source": [
    "# from numpy \n",
    "a = np.array([1, 1])\n",
    "a_t = t.from_numpy(a)\n",
    "print(\"Data type of a:{num}; Data type of a_t:{ten}\".format(num=a.dtype, ten=a_t.dtype)) # as we see, that the a_t follows the basic type of origin numpy data\n",
    "\n",
    "# from list, do not prefer to use this method!!!!!!\n",
    "b_ft = t.FloatTensor([2, 3.3])\n",
    "b_t = t.tensor([2, 3.3]) \n",
    "print(\"Data type of b_ft:{num}; Data type of b_t:{ten}\".format(num=b_ft.dtype, ten=b_t.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Creation using dimension, its values will be random generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.4176e-12, 1.7740e+28, 7.1447e+31],\n",
       "         [1.6216e-19, 7.0362e+22, 7.5632e+28]]),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 6.7943e+22],\n",
       "         [5.2669e-08, 6.6386e-07, 4.3126e-08]]),\n",
       " tensor([[0, 0, 0],\n",
       "         [0, 0, 0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation of tensor just use dimension as input\n",
    "a = t.empty(2, 3)\n",
    "b = t.FloatTensor(2, 3)\n",
    "c = t.IntTensor(2, 3)\n",
    "a,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Build up a tensor using the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a tensor using full function: tensor([[7, 7, 7],\n",
      "        [7, 7, 7]]) and its data type torch.int64\n",
      "Create an all zeros tensor: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]) and its data type torch.float32\n",
      "Create an all ones tensor: tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) and its data type torch.float32\n",
      "Create an unit diagnoal matrix tensor: tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]]) and its data type torch.float32\n"
     ]
    }
   ],
   "source": [
    "# full function, tensor data type is the same as the second input \n",
    "x = t.full([2, 3],7)\n",
    "print(\"Create a tensor using full function: {ten} and its data type {data_t}\".format(ten=x, data_t=x.dtype))\n",
    "\n",
    "# all zeros tensor, data type follows the pytorch default type \n",
    "x = t.zeros([2, 4])\n",
    "print(\"Create an all zeros tensor: {ten} and its data type {data_t}\".format(ten=x, data_t=x.dtype))\n",
    "\n",
    "# all ones tensor, data type follows the pytorch default type \n",
    "x = t.ones([2, 4])\n",
    "print(\"Create an all ones tensor: {ten} and its data type {data_t}\".format(ten=x, data_t=x.dtype))\n",
    "\n",
    "# unit diagonal matrix as tensor, data type follows the pytorch default type, dimension must bigger than 1\n",
    "x = t.eye(2, 4)\n",
    "print(\"Create an unit diagnoal matrix tensor: {ten} and its data type {data_t}\".format(ten=x, data_t=x.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3615],\n",
      "        [0.6256],\n",
      "        [0.7473]])\n",
      "tensor([[0.6256],\n",
      "        [0.3615],\n",
      "        [0.7473]])\n"
     ]
    }
   ],
   "source": [
    "a = t.rand(3, 1)\n",
    "print(a)\n",
    "\n",
    "# create a random idex array from 0 to 3\n",
    "idx = t.randperm(3)\n",
    "\n",
    "a_sf = a[idx]\n",
    "print(a_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor operation\n",
    "1.Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2915, 0.3833, 0.1934],\n",
      "        [0.6083, 0.7394, 0.6485],\n",
      "        [0.0284, 0.8289, 0.6710],\n",
      "        [0.8073, 0.8999, 0.9897],\n",
      "        [0.0389, 0.1359, 0.0736]])\n",
      "tensor([[0.5958, 0.0376, 0.7749],\n",
      "        [0.1711, 0.1967, 0.7865],\n",
      "        [0.4026, 0.2271, 0.0474],\n",
      "        [0.3513, 0.4790, 0.1851],\n",
      "        [0.9638, 0.6838, 0.5275]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8873, 0.4208, 0.9683],\n",
       "        [0.7794, 0.9361, 1.4350],\n",
       "        [0.4310, 1.0560, 0.7185],\n",
       "        [1.1586, 1.3788, 1.1748],\n",
       "        [1.0027, 0.8197, 0.6011]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.rand(5, 3)\n",
    "y = t.rand(5, 3)\n",
    "print(x,y,sep='\\n')\n",
    "\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8873, 0.4208, 0.9683],\n",
       "        [0.7794, 0.9361, 1.4350],\n",
       "        [0.4310, 1.0560, 0.7185],\n",
       "        [1.1586, 1.3788, 1.1748],\n",
       "        [1.0027, 0.8197, 0.6011]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = t.add(x, y)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between result: tensor([[0.8873, 0.4208, 0.9683],\n",
      "        [0.7794, 0.9361, 1.4350],\n",
      "        [0.4310, 1.0560, 0.7185],\n",
      "        [1.1586, 1.3788, 1.1748],\n",
      "        [1.0027, 0.8197, 0.6011]]) \n",
      " and y: tensor([[0.5958, 0.0376, 0.7749],\n",
      "        [0.1711, 0.1967, 0.7865],\n",
      "        [0.4026, 0.2271, 0.0474],\n",
      "        [0.3513, 0.4790, 0.1851],\n",
      "        [0.9638, 0.6838, 0.5275]])\n",
      "Comparison between result: tensor([[0.8873, 0.4208, 0.9683],\n",
      "        [0.7794, 0.9361, 1.4350],\n",
      "        [0.4310, 1.0560, 0.7185],\n",
      "        [1.1586, 1.3788, 1.1748],\n",
      "        [1.0027, 0.8197, 0.6011]]) \n",
      " and y: tensor([[0.8873, 0.4208, 0.9683],\n",
      "        [0.7794, 0.9361, 1.4350],\n",
      "        [0.4310, 1.0560, 0.7185],\n",
      "        [1.1586, 1.3788, 1.1748],\n",
      "        [1.0027, 0.8197, 0.6011]])\n"
     ]
    }
   ],
   "source": [
    "# object method addition\n",
    "result = y.add(x)\n",
    "print(\"Comparison between result: {res} \\n and y: {yy}\".format(res=result, yy=y)) # do not change the value of y \n",
    "\n",
    "# inplace addition\n",
    "result = y.add_(x)\n",
    "print(\"Comparison between result: {res} \\n and y: {yy}\".format(res=result, yy=y)) # the value of y is changed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4]) torch.Size([3, 4])\n",
      "a:= tensor([[0.9477, 0.8660, 0.9188, 0.4647],\n",
      "        [0.5913, 0.2223, 0.5526, 0.9539],\n",
      "        [0.8989, 0.9163, 0.8479, 0.5364]]);\n",
      " b:= tensor([0.5236, 0.2809, 0.0127, 0.2103]);\n",
      " c1:= tensor([[ 0.4241,  0.5850,  0.9061,  0.2544],\n",
      "        [ 0.0677, -0.0586,  0.5400,  0.7436],\n",
      "        [ 0.3753,  0.6353,  0.8352,  0.3261]]);\n",
      " c2:= tensor([[ 0.4241,  0.5850,  0.9061,  0.2544],\n",
      "        [ 0.0677, -0.0586,  0.5400,  0.7436],\n",
      "        [ 0.3753,  0.6353,  0.8352,  0.3261]])\n"
     ]
    }
   ],
   "source": [
    "a = t.rand(3, 4)\n",
    "b = t.rand(4)\n",
    "\n",
    "# boardcasting\n",
    "c1 = a - b\n",
    "c2 = t.sub(a, b)\n",
    "\n",
    "print(c1.shape, c2.shape)\n",
    "print(\"a:= {a};\\n b:= {b};\\n c1:= {c1};\\n c2:= {c2}\".format(a=a, b=b, c1=c1, c2=c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Element wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:= tensor([[0.9477, 0.8660, 0.9188, 0.4647],\n",
      "        [0.5913, 0.2223, 0.5526, 0.9539],\n",
      "        [0.8989, 0.9163, 0.8479, 0.5364]]);\n",
      " b:= tensor([0.5236, 0.2809, 0.0127, 0.2103]);\n",
      " c1:= tensor([[0.4963, 0.2433, 0.0116, 0.0977],\n",
      "        [0.3096, 0.0625, 0.0070, 0.2006],\n",
      "        [0.4707, 0.2574, 0.0107, 0.1128]]);\n",
      " c2:= tensor([[0.4963, 0.2433, 0.0116, 0.0977],\n",
      "        [0.3096, 0.0625, 0.0070, 0.2006],\n",
      "        [0.4707, 0.2574, 0.0107, 0.1128]])\n"
     ]
    }
   ],
   "source": [
    "c1 = a * b\n",
    "c2 = t.mul(a, b)\n",
    "print(\"a:= {a};\\n b:= {b};\\n c1:= {c1};\\n c2:= {c2}\".format(a=a, b=b, c1=c1, c2=c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Element wise division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4]) torch.Size([3, 4])\n",
      "a:= tensor([[0.9477, 0.8660, 0.9188, 0.4647],\n",
      "        [0.5913, 0.2223, 0.5526, 0.9539],\n",
      "        [0.8989, 0.9163, 0.8479, 0.5364]]);\n",
      " b:= tensor([0.5236, 0.2809, 0.0127, 0.2103]);\n",
      " c1:= tensor([[ 1.8099,  3.0822, 72.5329,  2.2100],\n",
      "        [ 1.1292,  0.7914, 43.6273,  4.5367],\n",
      "        [ 1.7167,  3.2614, 66.9348,  2.5512]]);\n",
      " c2:= tensor([[ 1.8099,  3.0822, 72.5329,  2.2100],\n",
      "        [ 1.1292,  0.7914, 43.6273,  4.5367],\n",
      "        [ 1.7167,  3.2614, 66.9348,  2.5512]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c1 = a / b\n",
    "c2 = t.div(a, b)\n",
    "print(c1.shape, c2.shape)\n",
    "print(\"a:= {a};\\n b:= {b};\\n c1:= {c1};\\n c2:= {c2}\".format(a=a, b=b, c1=c1, c2=c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "torch.Size([4, 3, 28, 32])\n"
     ]
    }
   ],
   "source": [
    "# 2D matrix multiplication\n",
    "\n",
    "a = t.ones(2, 1)\n",
    "b = t.ones(1, 2)\n",
    "print(t.mm(a, b))\n",
    "print(t.matmul(a, b))\n",
    "print(a @ b)\n",
    "\n",
    "# n-D matrix multipication, only the last two dimension will be multipled.\n",
    "\n",
    "c = t.rand(4, 3, 28, 64)\n",
    "d = t.rand(4, 3, 64, 32)\n",
    "print(t.matmul(c, d).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)\n",
    "y = y.to(x.device)\n",
    "z = x+y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "The algorithm of deep learning essentially uses backpropagation to determine the derivative, and PyTorch's autograd module implements this function. For all operations on Tensor, autograd can automatically provide differentiation for them, avoiding the complicated process of manually calculating derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torch.autograd in torch:\n",
      "\n",
      "NAME\n",
      "    torch.autograd\n",
      "\n",
      "DESCRIPTION\n",
      "    ``torch.autograd`` provides classes and functions implementing automatic\n",
      "    differentiation of arbitrary scalar valued functions. It requires minimal\n",
      "    changes to the existing code - you only need to declare :class:`Tensor` s\n",
      "    for which gradients should be computed with the ``requires_grad=True`` keyword.\n",
      "    As of now, we only support autograd for floating point :class:`Tensor` types (\n",
      "    half, float, double and bfloat16) and complex :class:`Tensor` types (cfloat, cdouble).\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _functions (package)\n",
      "    anomaly_mode\n",
      "    function\n",
      "    functional\n",
      "    grad_mode\n",
      "    gradcheck\n",
      "    profiler\n",
      "    variable\n",
      "\n",
      "CLASSES\n",
      "    torch._C._FunctionBase(builtins.object)\n",
      "        torch.autograd.function.Function(torch._C._FunctionBase, torch.autograd.function._ContextMethodMixin, torch.autograd.function._HookMixin)\n",
      "    torch._C._LegacyVariableBase(builtins.object)\n",
      "        torch.autograd.variable.Variable\n",
      "    torch.autograd.function._ContextMethodMixin(builtins.object)\n",
      "        torch.autograd.function.Function(torch._C._FunctionBase, torch.autograd.function._ContextMethodMixin, torch.autograd.function._HookMixin)\n",
      "    torch.autograd.function._HookMixin(builtins.object)\n",
      "        torch.autograd.function.Function(torch._C._FunctionBase, torch.autograd.function._ContextMethodMixin, torch.autograd.function._HookMixin)\n",
      "    \n",
      "    class Function(torch._C._FunctionBase, _ContextMethodMixin, _HookMixin)\n",
      "     |  Records operation history and defines formulas for differentiating ops.\n",
      "     |  \n",
      "     |  See the Note on extending the autograd engine for more details on how to use\n",
      "     |  this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\n",
      "     |  \n",
      "     |  Every operation performed on :class:`Tensor` s creates a new function\n",
      "     |  object, that performs the computation, and records that it happened.\n",
      "     |  The history is retained in the form of a DAG of functions, with edges\n",
      "     |  denoting data dependencies (``input <- output``). Then, when backward is\n",
      "     |  called, the graph is processed in the topological ordering, by calling\n",
      "     |  :func:`backward` methods of each :class:`Function` object, and passing\n",
      "     |  returned gradients on to next :class:`Function` s.\n",
      "     |  \n",
      "     |  Normally, the only way users interact with functions is by creating\n",
      "     |  subclasses and defining new operations. This is a recommended way of\n",
      "     |  extending torch.autograd.\n",
      "     |  \n",
      "     |  Examples::\n",
      "     |  \n",
      "     |      >>> class Exp(Function):\n",
      "     |      >>>\n",
      "     |      >>>     @staticmethod\n",
      "     |      >>>     def forward(ctx, i):\n",
      "     |      >>>         result = i.exp()\n",
      "     |      >>>         ctx.save_for_backward(result)\n",
      "     |      >>>         return result\n",
      "     |      >>>\n",
      "     |      >>>     @staticmethod\n",
      "     |      >>>     def backward(ctx, grad_output):\n",
      "     |      >>>         result, = ctx.saved_tensors\n",
      "     |      >>>         return grad_output * result\n",
      "     |      >>>\n",
      "     |      >>> #Use it by calling the apply method:\n",
      "     |      >>> output = Exp.apply(input)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Function\n",
      "     |      torch._C._FunctionBase\n",
      "     |      _ContextMethodMixin\n",
      "     |      _HookMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwargs)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  backward(ctx: Any, *grad_outputs: Any) -> Any\n",
      "     |      Defines a formula for differentiating the operation.\n",
      "     |      \n",
      "     |      This function is to be overridden by all subclasses.\n",
      "     |      \n",
      "     |      It must accept a context :attr:`ctx` as the first argument, followed by\n",
      "     |      as many outputs did :func:`forward` return, and it should return as many\n",
      "     |      tensors, as there were inputs to :func:`forward`. Each argument is the\n",
      "     |      gradient w.r.t the given output, and each returned value should be the\n",
      "     |      gradient w.r.t. the corresponding input.\n",
      "     |      \n",
      "     |      The context can be used to retrieve tensors saved during the forward\n",
      "     |      pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\n",
      "     |      of booleans representing whether each input needs gradient. E.g.,\n",
      "     |      :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\n",
      "     |      first input to :func:`forward` needs gradient computated w.r.t. the\n",
      "     |      output.\n",
      "     |  \n",
      "     |  forward(ctx: Any, *args: Any, **kwargs: Any) -> Any\n",
      "     |      Performs the operation.\n",
      "     |      \n",
      "     |      This function is to be overridden by all subclasses.\n",
      "     |      \n",
      "     |      It must accept a context ctx as the first argument, followed by any\n",
      "     |      number of arguments (tensors or other types).\n",
      "     |      \n",
      "     |      The context can be used to store tensors that can be then retrieved\n",
      "     |      during the backward pass.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  is_traceable = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  name(...)\n",
      "     |  \n",
      "     |  register_hook(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  apply(...) from torch.autograd.function.FunctionMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch._C._FunctionBase:\n",
      "     |  \n",
      "     |  dirty_tensors\n",
      "     |  \n",
      "     |  materialize_grads\n",
      "     |  \n",
      "     |  metadata\n",
      "     |  \n",
      "     |  needs_input_grad\n",
      "     |  \n",
      "     |  next_functions\n",
      "     |  \n",
      "     |  non_differentiable\n",
      "     |  \n",
      "     |  requires_grad\n",
      "     |  \n",
      "     |  saved_tensors\n",
      "     |  \n",
      "     |  saved_variables\n",
      "     |  \n",
      "     |  to_save\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _ContextMethodMixin:\n",
      "     |  \n",
      "     |  mark_dirty(self, *args)\n",
      "     |      Marks given tensors as modified in an in-place operation.\n",
      "     |      \n",
      "     |      **This should be called at most once, only from inside the**\n",
      "     |      :func:`forward` **method, and all arguments should be inputs.**\n",
      "     |      \n",
      "     |      Every tensor that's been modified in-place in a call to :func:`forward`\n",
      "     |      should be given to this function, to ensure correctness of our checks.\n",
      "     |      It doesn't matter whether the function is called before or after\n",
      "     |      modification.\n",
      "     |  \n",
      "     |  mark_non_differentiable(self, *args)\n",
      "     |      Marks outputs as non-differentiable.\n",
      "     |      \n",
      "     |      **This should be called at most once, only from inside the**\n",
      "     |      :func:`forward` **method, and all arguments should be outputs.**\n",
      "     |      \n",
      "     |      This will mark outputs as not requiring gradients, increasing the\n",
      "     |      efficiency of backward computation. You still need to accept a gradient\n",
      "     |      for each output in :meth:`~Function.backward`, but it's always going to\n",
      "     |      be a zero tensor with the same shape as the shape of a corresponding\n",
      "     |      output.\n",
      "     |      \n",
      "     |      This is used e.g. for indices returned from a max :class:`Function`.\n",
      "     |  \n",
      "     |  mark_shared_storage(self, *pairs)\n",
      "     |  \n",
      "     |  save_for_backward(self, *tensors)\n",
      "     |      Saves given tensors for a future call to :func:`~Function.backward`.\n",
      "     |      \n",
      "     |      **This should be called at most once, and only from inside the**\n",
      "     |      :func:`forward` **method.**\n",
      "     |      \n",
      "     |      Later, saved tensors can be accessed through the :attr:`saved_tensors`\n",
      "     |      attribute. Before returning them to the user, a check is made to ensure\n",
      "     |      they weren't used in any in-place operation that modified their content.\n",
      "     |      \n",
      "     |      Arguments can also be ``None``.\n",
      "     |  \n",
      "     |  set_materialize_grads(self, value)\n",
      "     |      Sets whether to materialize output grad tensors. Default is true.\n",
      "     |      \n",
      "     |      **This should be called only from inside the** :func:`forward` **method**\n",
      "     |      \n",
      "     |      If true, undefined output grad tensors will be expanded to tensors full\n",
      "     |      of zeros prior to calling the :func:`backward` method.\n",
      "    \n",
      "    class Variable(torch._C._LegacyVariableBase)\n",
      "     |  # mypy doesn't understand torch._six.with_metaclass\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Variable\n",
      "     |      torch._C._LegacyVariableBase\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch._C._LegacyVariableBase:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n",
      "FUNCTIONS\n",
      "    backward(tensors: Union[torch.Tensor, Sequence[torch.Tensor]], grad_tensors: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None, retain_graph: Union[bool, NoneType] = None, create_graph: bool = False, grad_variables: Union[torch.Tensor, Sequence[torch.Tensor], NoneType] = None) -> None\n",
      "        Computes the sum of gradients of given tensors w.r.t. graph leaves.\n",
      "        \n",
      "        The graph is differentiated using the chain rule. If any of ``tensors``\n",
      "        are non-scalar (i.e. their data has more than one element) and require\n",
      "        gradient, then the Jacobian-vector product would be computed, in this\n",
      "        case the function additionally requires specifying ``grad_tensors``.\n",
      "        It should be a sequence of matching length, that contains the \"vector\"\n",
      "        in the Jacobian-vector product, usually the gradient of the differentiated\n",
      "        function w.r.t. corresponding tensors (``None`` is an acceptable value for\n",
      "        all tensors that don't need gradient tensors).\n",
      "        \n",
      "        This function accumulates gradients in the leaves - you might need to zero\n",
      "        ``.grad`` attributes or set them to ``None`` before calling it.\n",
      "        See :ref:`Default gradient layouts<default-grad-layouts>`\n",
      "        for details on the memory layout of accumulated gradients.\n",
      "        \n",
      "        .. note::\n",
      "            Using this method with ``create_graph=True`` will create a reference cycle\n",
      "            between the parameter and its gradient which can cause a memory leak.\n",
      "            We recommend using ``autograd.grad`` when creating the graph to avoid this.\n",
      "            If you have to use this function, make sure to reset the ``.grad`` fields of your\n",
      "            parameters to ``None`` after use to break the cycle and avoid the leak.\n",
      "        \n",
      "        Arguments:\n",
      "            tensors (sequence of Tensor): Tensors of which the derivative will be\n",
      "                computed.\n",
      "            grad_tensors (sequence of (Tensor or None)): The \"vector\" in the Jacobian-vector\n",
      "                product, usually gradients w.r.t. each element of corresponding tensors.\n",
      "                None values can be specified for scalar Tensors or ones that don't require\n",
      "                grad. If a None value would be acceptable for all grad_tensors, then this\n",
      "                argument is optional.\n",
      "            retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n",
      "                will be freed. Note that in nearly all cases setting this option to ``True``\n",
      "                is not needed and often can be worked around in a much more efficient\n",
      "                way. Defaults to the value of ``create_graph``.\n",
      "            create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      "                be constructed, allowing to compute higher order derivative products.\n",
      "                Defaults to ``False``.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Variable', 'Function', 'backward', 'grad_mode']\n",
      "\n",
      "FILE\n",
      "    d:\\anaconda\\envs\\pytorch_gqu\\lib\\site-packages\\torch\\autograd\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overview the description of autograd\n",
    "help(t.autograd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use torch.autograd to calculate the gradient, when a neutral network is built up using pytorch. To get the gradient information, we follow the next steps:\n",
    "1. Build up a calculation graph using torch.tensor with requires_grad=True. The tensors form the calculation node. \n",
    "2. After finishing all the tensor calculation, execute .backward() to calculate the needed gradient.\n",
    "3. Use torch.tensor.grad to get the gradient of a certain tensor or variable.\n",
    "\n",
    "Under the context of gradient, torch.tensor has two important characteristics, requires_grad and grad_fn Both characteristics have something to do with manual operation.\n",
    "1. requires_grad has a boolean type. True means that the tensor's gradient needs to be calculated. False, just in the opposite. The requires_grad must be set when the tensor is built up, its default value is false.\n",
    "2. grad_fn return if the tensor is result of a calculation or a function, for example torch.matmul(), torch.mul(), torch.add(). If yes, return the type of the calculation. \n",
    "\n",
    "To understand \"requires_grad\" and \"grad_fn\" better, there is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False True False True\n",
      "None None None None <AddBackward0 object at 0x0000023295860CD0>\n"
     ]
    }
   ],
   "source": [
    "# build up the calculation map\n",
    "x = t.randn(2, 2)\n",
    "y = t.randn(2, 2)\n",
    "z = t.randn(2, 2,requires_grad=True)\n",
    "a = x + y\n",
    "b = a + z\n",
    "\n",
    "print(x.requires_grad, y.requires_grad, z.requires_grad, a.requires_grad, b.requires_grad)\n",
    "print(x.grad_fn, y.grad_fn, z.grad_fn, a.grad_fn, b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/01.png)\n",
    "The graph show the calculation map. In pytorch we define x, y and z as \"leaf variables\". It is easy to understand the results of x, y, z's requires_grad. As b is a calculation result involving z, its requires_grad is also True. grad_fn is a little bit special, only a tensor signed with requires_grad=True, will be considered.Besides all tensors and their derived calculation results' grad_fn will be signed as None. If we modify x's requires_grad as True, a.grad_fn will change to be <AddBackward0 object at ....>. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False True True True\n",
      "None None None <AddBackward0 object at 0x00000232E83E9400> <AddBackward0 object at 0x00000232E83E95B0>\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad = True\n",
    "a = x + y\n",
    "print(x.requires_grad, y.requires_grad, z.requires_grad, a.requires_grad, b.requires_grad)\n",
    "print(x.grad_fn, y.grad_fn, z.grad_fn, a.grad_fn, b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Pay attention! Only the leaf variables' requires_grad can be modified. If we write \"a.requires_grad = True\", it declares an error.</font>\n",
    "![title](img/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the gradient calculation will be executed in pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3.],\n",
       "         [4., 5., 6.]], requires_grad=True),\n",
       " tensor([[3., 4., 5.],\n",
       "         [6., 7., 8.]], grad_fn=<AddBackward0>),\n",
       " tensor([[ 27.,  48.,  75.],\n",
       "         [108., 147., 192.]], grad_fn=<MulBackward0>),\n",
       " tensor(99.5000, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do forward propagation\n",
    "x = t.tensor([[1.,2.,3.],[4.,5.,6.]], requires_grad=True)\n",
    "y = x + 2\n",
    "z = t.pow(y, 2) * 3\n",
    "out = z.mean()\n",
    "x, y, z, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4., 5.],\n",
       "        [6., 7., 8.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just run this block for 1 time. Otherwise the result is false! We will explain it later\n",
    "out.backward() \n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation map here is really easy to understand, x -> y -> z -> out. We can see that out.backward() means to calculate $\\frac{\\partial out}{\\partial x_i}$. Here we ignore the mathmatical derivation.\n",
    "\n",
    "As we know that out determine the mean of z, $out=\\frac{1}{dim(z)}\\sum_i z_i$, so that out here is a scalar. <font color=red>If we want to know the derivation of z to x, the gradient parameter must be set as a ones tensor with the same dimension as z. Otherwise it will declare an error called: RuntimeError: grad can be implicitly created only for scalar outputs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18., 24., 30.],\n",
       "        [36., 42., 48.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients = t.ones(z.size())\n",
    "z.backward(gradients)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason of, why we need an gradients parameter, is that pytorch only allow to calculated the derivation from scalar to tensor. From tensor to tensor is really hard to realize, so pytorch design a mechanism that z.backward(gradient) actually means:\n",
    "```python\n",
    "L = torch.sum(z * gradient)\n",
    "L.backward()\n",
    "x.grad\n",
    "```\n",
    "Now there is another question: how can we get the derivation of a median value, for example $\\frac{\\partial out}{\\partial y}$. In the debugging process, sometimes we need to monitor the mediate variable gradient to ensure the effectiveness of the network. At this time, we need to print out the gradient of the non-leaf node.Here we introduce one method, retain_grad. The other one register_hook, see https://www.jianshu.com/p/ad66f2e38f2fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = t.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "y.retain_grad()\n",
    "z = t.pow(y, 2) * 3\n",
    "out = z.mean()\n",
    "out.backward()\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to solve the initial operational problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do forward propagation\n",
    "x = t.tensor([[1.,2.,3.],[4.,5.,6.]], requires_grad=True)\n",
    "y = x + 2\n",
    "z = t.pow(y, 2) * 3\n",
    "out = z.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we run the following block for many time. An error occurs, <font color= red>Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRun the following code twice if you want to see the error!\\n-----------------------------------------------------------\\nout.backward() \\nx.grad\\n\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Run the following code twice if you want to see the error!\n",
    "-----------------------------------------------------------\n",
    "out.backward() \n",
    "x.grad\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem,we set the parameter retain_graph as True. Now we can run this code many time as you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 20., 25.],\n",
       "        [30., 35., 40.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "If you have run the block above before you reach here, run firstly the block \"do forward propagation\" above and then run the code below many\n",
    "times. Focus on the result, you will find something interesting!\n",
    "\n",
    "'''\n",
    "out.backward(retain_graph=True) \n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, anothor problem comes out. We can see from the results, as the number of runs increases, the gradient calculation results will add up. <font color=red>Therefore, remember to clear the gradient information of x every time you run!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4., 5.],\n",
       "        [6., 7., 8.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.data.zero_() # clear x's gradient information\n",
    "out.backward(retain_graph=True) \n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
